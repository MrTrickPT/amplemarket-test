{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA INGESTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "file_path = \"../Data_Science_Challenge_Data - Sheet1.csv\"\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-04 22:09:58.421235: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/filipe.marques/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/var/folders/pf/gd1v7f115p991jrc42_n88kw0000gp/T/ipykernel_8153/2606707356.py:6: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna(\"\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "# Replace NaN values in 'technologies' column\n",
    "df.fillna(\"\", inplace=True)\n",
    "# df['technologies'] = df['technologies'].replace('nan', '')\n",
    "\n",
    "# Encode the 'type' column\n",
    "type_map = {\"B2B\": 0, \"B2C\": 1, \"B2B B2C\": 2, \"None\": 3}\n",
    "df[\"type_encoded\"] = df[\"type\"].map(type_map)\n",
    "\n",
    "\n",
    "# Tokenize and preprocess\n",
    "embedding_columns = [\n",
    "    \"technologies\",\n",
    "    \"specialties\",\n",
    "    \"company_hubs\",\n",
    "    \"industry\",\n",
    "    \"categories\",\n",
    "]\n",
    "\n",
    "# convert size to small medium big to test it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(\n",
    "    df,\n",
    "    column_names,\n",
    "    vector_size=5,\n",
    "    window=3,\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    "    max_sequence_length=5,\n",
    "):\n",
    "    def _preprocess(entry):\n",
    "        words = [word.strip().lower() for word in entry.split(\",\")]\n",
    "        filtered_words = [word for word in words if word not in stop_words]\n",
    "        return filtered_words\n",
    "\n",
    "    # Tokenize and preprocess for all specified columns\n",
    "    for column_name in column_names:\n",
    "        df[f\"tokenized_{column_name}\"] = df[column_name].apply(_preprocess)\n",
    "\n",
    "    # Combine all tokenized columns into a single list of sentences\n",
    "    all_sentences = df[\n",
    "        [f\"tokenized_{column_name}\" for column_name in column_names]\n",
    "    ].values.flatten()\n",
    "\n",
    "    # Create Word2Vec model\n",
    "    model = Word2Vec(\n",
    "        sentences=all_sentences,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=workers,\n",
    "    )\n",
    "\n",
    "    for column_name in column_names:\n",
    "        embeddings = df[f\"tokenized_{column_name}\"].apply(\n",
    "            lambda entry: [model.wv[word] for word in entry if word in model.wv]\n",
    "        )\n",
    "\n",
    "        # Pad sequences\n",
    "        padded_embeddings = pad_sequences(\n",
    "            embeddings,\n",
    "            maxlen=max_sequence_length,\n",
    "            dtype=\"float32\",\n",
    "            padding=\"post\",\n",
    "            truncating=\"post\",\n",
    "        )\n",
    "\n",
    "        # Flatten the embeddings\n",
    "        flattened_embeddings = pd.DataFrame(\n",
    "            padded_embeddings.reshape(-1, vector_size * max_sequence_length),\n",
    "            columns=[\n",
    "                f\"{column_name}_feature_{i+1}\"\n",
    "                for i in range(vector_size * max_sequence_length)\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Concatenate the new DataFrame with your original DataFrame\n",
    "        df = pd.concat([df, flattened_embeddings], axis=1)\n",
    "\n",
    "    return df, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, embedings_model = create_embeddings(df, embedding_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../saved_models/word2vec_model_latest.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save Word2Vec model with the current date in the filename\n",
    "model_filename = f\"../saved_models/word2vec_model_latest.pkl\"\n",
    "\n",
    "joblib.dump(embedings_model, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = df[[f\"{column}_feature_{i+1}\" for column in embedding_columns for i in range(25)]]\n",
    "y = df[\"type_encoded\"]\n",
    "\n",
    "# Split the data into training and evaluation sets\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML MODEL TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7416666666666667\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      1.00      0.85       267\n",
      "           1       0.00      0.00      0.00        33\n",
      "           2       0.00      0.00      0.00        60\n",
      "\n",
      "    accuracy                           0.74       360\n",
      "   macro avg       0.25      0.33      0.28       360\n",
      "weighted avg       0.55      0.74      0.63       360\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/filipe.marques/miniconda3-intel/envs/amt/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/filipe.marques/miniconda3-intel/envs/amt/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/filipe.marques/miniconda3-intel/envs/amt/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize the SVM model\n",
    "svm_model = SVC(kernel=\"linear\", C=1.0)\n",
    "\n",
    "# Train the SVM model on the training set\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the evaluation set\n",
    "y_pred = svm_model.predict(X_eval)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_eval, y_pred)\n",
    "classification_rep = classification_report(y_eval, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 0.1, 'kernel': 'poly'}\n",
      "Accuracy: 0.7305555555555555\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.96      0.85       267\n",
      "           1       0.14      0.03      0.05        33\n",
      "           2       0.38      0.10      0.16        60\n",
      "\n",
      "    accuracy                           0.73       360\n",
      "   macro avg       0.43      0.36      0.35       360\n",
      "weighted avg       0.64      0.73      0.66       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the SVM model\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "svm_model = SVC(class_weight=dict(enumerate(class_weights)))\n",
    "\n",
    "# Set up parameter grid for grid search\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10], 'kernel': ['linear', 'rbf', 'poly']}\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Initialize the GridSearchCV\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_eval_scaled = scaler.transform(X_eval)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best model from grid search\n",
    "best_svm_model = grid_search.best_estimator_\n",
    "\n",
    "# Predictions on the evaluation set (use the scaled data)\n",
    "y_pred = best_svm_model.predict(X_eval_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_eval, y_pred)\n",
    "classification_rep = classification_report(y_eval, y_pred)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model:\n",
      "Best Hyperparameters: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_depth': 30}\n",
      "Accuracy: 0.8333333333333334\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.99      0.90       267\n",
      "           1       0.60      0.09      0.16        33\n",
      "           2       1.00      0.53      0.70        60\n",
      "\n",
      "    accuracy                           0.83       360\n",
      "   macro avg       0.81      0.54      0.58       360\n",
      "weighted avg       0.83      0.83      0.80       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming you have X and y defined\n",
    "# Split the data into training and evaluation sets\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_eval_scaled = scaler.transform(X_eval)\n",
    "\n",
    "# Initialize the Random Forest model with class weights\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "rf_model = RandomForestClassifier(class_weight='balanced', n_estimators=100, random_state=42)\n",
    "\n",
    "# Set up parameter distributions for randomized search\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 150, 200, 250, 300, 350, 400],\n",
    "    'max_depth': [None, 10, 20, 30, 40],\n",
    "    'min_samples_split': [2, 5, 10, 20, 30],\n",
    "    'min_samples_leaf': [1, 2, 4, 8]\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "rf_random_search = RandomizedSearchCV(rf_model, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "\n",
    "# Perform randomized search to find optimal hyperparameters\n",
    "rf_random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best Random Forest model from the search\n",
    "best_rf_model = rf_random_search.best_estimator_\n",
    "\n",
    "# Predictions on the evaluation set (use the scaled data)\n",
    "y_pred_rf = best_rf_model.predict(X_eval_scaled)\n",
    "\n",
    "# Evaluate the Random Forest model\n",
    "accuracy_rf = accuracy_score(y_eval, y_pred_rf)\n",
    "classification_rep_rf = classification_report(y_eval, y_pred_rf)\n",
    "\n",
    "# Print results\n",
    "print(\"Random Forest Model:\")\n",
    "print(f\"Best Hyperparameters: {rf_random_search.best_params_}\")\n",
    "print(f\"Accuracy: {accuracy_rf}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 2 0 0 0 0 0 0 2 0 0 0 2 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as ../saved_models/rf_model_latest.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained SVM model to a file\n",
    "model_filename = \"../saved_models/rf_model_latest.pkl\"\n",
    "joblib.dump(best_rf_model, model_filename)\n",
    "\n",
    "print(f\"Model saved as {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
